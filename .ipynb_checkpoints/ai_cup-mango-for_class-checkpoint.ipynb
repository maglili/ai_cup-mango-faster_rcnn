{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意\n",
    "本程式使用pytorch提供的Faster RCNN模型<br>\n",
    "訓練整個Train資料匣，需要花費22小時(GPU: RTX-2070 Super，CPU: i7-10700)<br>\n",
    "訓練時請確定vram有7G以上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pycocotools) (3.3.2)\n",
      "Requirement already satisfied: cython>=0.27.3 in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pycocotools) (0.29.21)\n",
      "Requirement already satisfied: setuptools>=18.0 in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pycocotools) (49.2.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (1.18.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\exia\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.50.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\user\\\\Documents\\\\Projects\\\\AICUP_2020_Mango'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 43370\n",
      "image: 43370\n",
      "xmin: 43370\n",
      "ymin: 43370\n",
      "xmax: 43370\n",
      "ymax: 43370\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "label=[]\n",
    "image=[]\n",
    "xmin=[]\n",
    "ymin=[]\n",
    "xmax=[]\n",
    "ymax=[]\n",
    "\n",
    "with open(r'.\\C2_TrainDev\\train.csv', 'r', encoding='utf-8-sig') as fh:\n",
    "  for index,row in enumerate(fh):\n",
    "    row=row.strip()\n",
    "    row=row.split(',')\n",
    "    while True:\n",
    "      if '' in row:\n",
    "        row.remove('')\n",
    "      else:\n",
    "        break\n",
    "\n",
    "    while len(row) > 1:\n",
    "      xmin.append(float(row[1]))\n",
    "      ymin.append(float(row[2]))\n",
    "      xmax.append(float(row[3])+float(row[1]))\n",
    "      ymax.append(float(row[4])+float(row[2]))\n",
    "      label.append(row[5])\n",
    "      for idx in range(5):\n",
    "        row.remove(row[1]) \n",
    "      image.append(row[0])\n",
    "\n",
    "print('label:' ,len(label))\n",
    "print('image:' ,len(image))\n",
    "print('xmin:' ,len(xmin))\n",
    "print('ymin:' ,len(ymin))\n",
    "print('xmax:' ,len(xmax))\n",
    "print('ymax:' ,len(ymax))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38414.jpg</td>\n",
       "      <td>不良-機械傷害</td>\n",
       "      <td>46.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03182.jpg</td>\n",
       "      <td>不良-機械傷害</td>\n",
       "      <td>581.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>370.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29863.jpg</td>\n",
       "      <td>不良-機械傷害</td>\n",
       "      <td>514.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17937.jpg</td>\n",
       "      <td>不良-機械傷害</td>\n",
       "      <td>658.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17937.jpg</td>\n",
       "      <td>不良-著色不佳</td>\n",
       "      <td>374.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43365</th>\n",
       "      <td>06815.jpg</td>\n",
       "      <td>不良-炭疽病</td>\n",
       "      <td>463.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43366</th>\n",
       "      <td>06815.jpg</td>\n",
       "      <td>不良-炭疽病</td>\n",
       "      <td>793.0</td>\n",
       "      <td>686.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43367</th>\n",
       "      <td>06815.jpg</td>\n",
       "      <td>不良-炭疽病</td>\n",
       "      <td>535.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>568.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43368</th>\n",
       "      <td>11562.jpg</td>\n",
       "      <td>不良-炭疽病</td>\n",
       "      <td>359.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>469.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43369</th>\n",
       "      <td>11562.jpg</td>\n",
       "      <td>不良-炭疽病</td>\n",
       "      <td>579.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43370 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           image    label   xmin   ymin    xmax   ymax\n",
       "0      38414.jpg  不良-機械傷害   46.0  146.0   622.0  720.0\n",
       "1      03182.jpg  不良-機械傷害  581.0  277.0   678.0  370.0\n",
       "2      29863.jpg  不良-機械傷害  514.0  538.0   631.0  682.0\n",
       "3      17937.jpg  不良-機械傷害  658.0  263.0   717.0  337.0\n",
       "4      17937.jpg  不良-著色不佳  374.0  243.0   983.0  577.0\n",
       "...          ...      ...    ...    ...     ...    ...\n",
       "43365  06815.jpg   不良-炭疽病  463.0   75.0  1172.0  543.0\n",
       "43366  06815.jpg   不良-炭疽病  793.0  686.0   828.0  709.0\n",
       "43367  06815.jpg   不良-炭疽病  535.0  540.0   579.0  568.0\n",
       "43368  11562.jpg   不良-炭疽病  359.0  434.0   404.0  469.0\n",
       "43369  11562.jpg   不良-炭疽病  579.0  149.0   638.0  175.0\n",
       "\n",
       "[43370 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = pd.DataFrame({'image':image, 'label':label, 'xmin':xmin, 'ymin':ymin, 'xmax':xmax, 'ymax':ymax  })\n",
    "traindf.to_pickle(r\".\\C2_TrainDev\\traindf.pkl\")\n",
    "traindf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### call pytorch-vision module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\user\\Documents\\Projects\\AICUP_2020_Mango\\pytorch_object_detection\\vision\\references\\detection\n"
     ]
    }
   ],
   "source": [
    "cd .\\pytorch_object_detection\\vision\\references\\detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_annot(image, train_state=True):\n",
    "  if train_state:\n",
    "    boxes_array = traindf[traindf[\"image\"] == image][[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values\n",
    "  else:\n",
    "    boxes_array = validdf[validdf[\"image\"] == image][[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values\n",
    "  return boxes_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, transforms=None):\n",
    "    self.root = root\n",
    "    self.transforms = transforms\n",
    "    self.imgs = sorted(os.listdir(os.path.join(root, \"Train\")))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # load images and bounding boxes\n",
    "    img_path = os.path.join(self.root, \"Train\", self.imgs[idx])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    box_list = parse_one_annot(self.imgs[idx], True)\n",
    "    boxes = torch.as_tensor(box_list, dtype=torch.float32)\n",
    "    num_objs = len(box_list)\n",
    "\n",
    "    # there is only one class******************************************************\n",
    "    labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "    for row in range(len(traindf[traindf['image'] == self.imgs[idx]])):\n",
    "      if traindf[traindf['image'] == self.imgs[idx]]['label'].iloc[row] == '不良-乳汁吸附':\n",
    "        labels[row] = 1\n",
    "      elif traindf[traindf['image'] == self.imgs[idx]]['label'].iloc[row] == '不良-機械傷害':\n",
    "        labels[row] = 2\n",
    "      elif traindf[traindf['image'] == self.imgs[idx]]['label'].iloc[row] == '不良-炭疽病': \n",
    "        labels[row] = 3\n",
    "      elif traindf[traindf['image'] == self.imgs[idx]]['label'].iloc[row] == '不良-著色不佳':\n",
    "        labels[row] = 4\n",
    "      else:\n",
    "        labels[row] = 5\n",
    "      \n",
    "    image_id = torch.tensor([idx])\n",
    "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:,0])\n",
    "\n",
    "    # suppose all instances are not crowd\n",
    "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "    target = {}\n",
    "    target[\"boxes\"] = boxes\n",
    "    target[\"labels\"] = labels\n",
    "    target[\"image_id\"] = image_id\n",
    "    target[\"area\"] = area\n",
    "    target[\"iscrowd\"] = iscrowd\n",
    "    if self.transforms is not None:\n",
    "      img, target = self.transforms(img, target)\n",
    "    return img, target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=1280x720 at 0x2AB85BEBFD0>,\n",
       " {'boxes': tensor([[566., 335., 637., 384.],\n",
       "          [476., 166., 753., 316.]]),\n",
       "  'labels': tensor([3, 4]),\n",
       "  'image_id': tensor([32]),\n",
       "  'area': tensor([ 3479., 41550.]),\n",
       "  'iscrowd': tensor([0, 0])})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = TrainDataset(root= \".\\..\\..\\..\\..\\C2_TrainDev\")\n",
    "train_set[32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, root, transforms=None):\n",
    "    self.root = root\n",
    "    self.transforms = transforms\n",
    "    self.imgs = sorted(os.listdir(os.path.join(root, \"Dev\")))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # load images and bounding boxes\n",
    "    img_path = os.path.join(self.root, \"Dev\", self.imgs[idx])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    target = {}\n",
    "    if self.transforms is not None:\n",
    "      img, target = self.transforms(img, target)\n",
    "    return img, target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=1280x720 at 0x2AB85B5EC40>, {})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set = ValidDataset(root= r\".\\..\\..\\..\\..\\C2_TrainDev\")\n",
    "valid_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  25768\n",
      "valid:  3681\n"
     ]
    }
   ],
   "source": [
    "print('train: ', len(train_set))\n",
    "print('valid: ', len(valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Download and adjust the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "  # load an object detection model pre-trained on COCO\n",
    "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "  # get the number of input features for the classifier\n",
    "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "  # replace the pre-trained head with a new on\n",
    "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n",
    "   \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "  transforms = []\n",
    "  # converts the image, a PIL image, into a PyTorch Tensor\n",
    "  transforms.append(T.ToTensor())\n",
    "  if train:\n",
    "    # during training, randomly flip the training images\n",
    "    # and ground-truth for data augmentation\n",
    "    transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "  return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "train_set = TrainDataset(root= r\".\\..\\..\\..\\..\\C2_TrainDev\",transforms = get_transform(train=True))\n",
    "\n",
    "valid_set = ValidDataset(root= r\".\\..\\..\\..\\..\\C2_TrainDev\", transforms = get_transform(train=False))\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)\n",
    "#data_loader_test = torch.utils.data.DataLoader(valid_set, batch_size=1, shuffle=False, collate_fn=utils.collate_fn)\n",
    "#print(\"We have: {} examples,  {} are training and {} testing\".format(len(train_set)+len(valid_set) ,len(train_set), len(valid_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: <function get_device_name at 0x000002AB84509CA0>\n",
      "-Training on GPU-\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('GPU:',torch.cuda.get_device_name)\n",
    "    print('-Training on GPU-')\n",
    "else:\n",
    "    print('GPU is not avalible!')\n",
    "    print('-Training on CPU-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# our dataset has two classes only - raccoon and not racoon****************************************\n",
    "num_classes = 6\n",
    "# get the model using our helper function\n",
    "model = get_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意\n",
    "若要自行**訓練**，請將下列cell 取消comment。<br>\n",
    "請確定目前VRAM有7G以上空間。<br>\n",
    "本模型在Dev_set上需訓練**22**小時。<br>\n",
    "請確保執行期間程式不會中斷。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_epochs = 10\\nfor epoch in range(num_epochs):\\n   # train for one epoch, printing every 10 iterations\\n   train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10) \\n   \\n   lr_scheduler.step() # update the learning rate\\n   \\n   \\ntry:\\n    os.mkdir(r\".\\\\..\\\\..\\\\..\\\\..\\\\pytorch_object_detection\\\\mango\")\\nexcept:\\n    print(\\'folder already exists\\')\\n    pass\\ntorch.save(model.state_dict(), r\".\\\\..\\\\..\\\\..\\\\..\\\\pytorch_object_detection\\\\mango\\\\model\")\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "   # train for one epoch, printing every 10 iterations\n",
    "   train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10) \n",
    "   \n",
    "   lr_scheduler.step() # update the learning rate\n",
    "   \n",
    "   \n",
    "try:\n",
    "    os.mkdir(r\".\\..\\..\\..\\..\\pytorch_object_detection\\mango\")\n",
    "except:\n",
    "    print('folder already exists')\n",
    "    pass\n",
    "torch.save(model.state_dict(), r\".\\..\\..\\..\\..\\pytorch_object_detection\\mango\\model\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = get_model(num_classes = 6)\n",
    "loaded_model.load_state_dict(torch.load(r\".\\..\\..\\..\\..\\pytorch_object_detection\\mango\\model\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[669.2596,  94.7342, 919.8629, 238.2311],\n",
       "          [768.4798, 105.7732, 919.2456, 217.5965],\n",
       "          [579.9876,  79.0225, 938.9802, 321.1524],\n",
       "          [711.0389,  91.0686, 914.0922, 181.7177],\n",
       "          [738.6102, 106.7090, 937.7908, 299.9219],\n",
       "          [707.1357, 111.1760, 871.0986, 274.4806],\n",
       "          [803.0746, 119.9626, 930.3448, 266.8000],\n",
       "          [822.3150, 125.4432, 917.7075, 213.4774]]),\n",
       "  'labels': tensor([4, 4, 4, 4, 4, 4, 4, 4]),\n",
       "  'scores': tensor([0.8179, 0.5821, 0.1587, 0.1091, 0.1013, 0.0934, 0.0631, 0.0543])}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "img,_ = valid_set[idx]\n",
    "\n",
    "#put the model in evaluation mode\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "   prediction = loaded_model([img])\n",
    "image = Image.fromarray(img.mul(255).permute(1, 2,0).byte().numpy())\n",
    "draw = ImageDraw.Draw(image)\n",
    "# draw groundtruth\n",
    "\n",
    "for element in range(len(prediction[0][\"boxes\"])):\n",
    "   boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
    "   score = np.round(prediction[0][\"scores\"][element].cpu().numpy(),\n",
    "                    decimals= 4)\n",
    "   if score > 0.5:\n",
    "      draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])], \n",
    "      outline =\"red\", width =3)\n",
    "      draw.text((boxes[0], boxes[1]), text = str(score))\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root= r\".\\..\\..\\..\\..\\C2_TrainDev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = sorted(os.listdir(os.path.join(root, \"Dev\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.eval()\n",
    "files=[]\n",
    "#label_map = {'不良-乳汁吸附':1, '不良-機械傷害':2, '不良-炭疽病':3, '不良-著色不佳':4, '不良-黑斑病':5}\n",
    "class1=[]\n",
    "class2=[]\n",
    "class3=[]\n",
    "class4=[]\n",
    "class5=[]\n",
    "\n",
    "for i in tqdm(range(len(imgs)), position=0, leave=True):\n",
    "    idx = i\n",
    "    img, _ = valid_set[idx]\n",
    "    label_boxes = np.array(valid_set[idx][1][\"boxes\"])\n",
    "                                                  \n",
    "    aa=bb=cc=dd=ee=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = loaded_model([img])    \n",
    "                           \n",
    "    for element in range(len(prediction[0][\"boxes\"])):\n",
    "        score = np.round(prediction[0][\"scores\"][element].numpy(), decimals= 4)\n",
    "        \n",
    "        if score > 0.5:\n",
    "            if prediction[0][\"labels\"][element].numpy() == 1:\n",
    "                aa=1\n",
    "            elif prediction[0][\"labels\"][element].numpy() == 2:\n",
    "                bb=1\n",
    "            elif prediction[0][\"labels\"][element].numpy() == 3:\n",
    "                cc=1\n",
    "            elif prediction[0][\"labels\"][element].numpy() == 4:               \n",
    "                dd=1\n",
    "            elif prediction[0][\"labels\"][element].numpy() == 5:\n",
    "                ee=1\n",
    "        else:\n",
    "            break\n",
    "                           \n",
    "    if aa == 1:\n",
    "        class1.append(True)\n",
    "    else:\n",
    "        class1.append(False)\n",
    "\n",
    "    if bb == 1:\n",
    "        class2.append(True)\n",
    "    else:\n",
    "        class2.append(False)\n",
    "\n",
    "    if cc == 1:\n",
    "        class3.append(True)\n",
    "    else:\n",
    "        class3.append(False)\n",
    "\n",
    "    if dd == 1:\n",
    "        class4.append(True)\n",
    "    else:\n",
    "        class4.append(False)\n",
    "\n",
    "    if ee == 1:\n",
    "        class5.append(True)\n",
    "    else:\n",
    "        class5.append(False)\n",
    "    \n",
    "    files.append(imgs[i])\n",
    "                           \n",
    "\n",
    "pred_df = pd.DataFrame({'image_id':files,'d0':class1,'d1':class2, 'd2':class3,  'd3':class4, 'd4':class5 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_pickle(r\".\\..\\..\\..\\..\\C2_TrainDev\\N26091194_predict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(r\".\\..\\..\\..\\..\\C2_TrainDev\\N26091194_predict.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
